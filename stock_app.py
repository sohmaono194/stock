import streamlit as st
import requests
import zipfile
import io
import xml.etree.ElementTree as ET
import pandas as pd
import chardet
import os
from datetime import datetime, timedelta

# --- APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆStreamlit Cloudã§ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ç›´æ¥å–å¾—ï¼‰ ---
API_KEY = os.environ.get("EDINET_API_KEY")

st.title("ğŸ“„ EDINETæå‡ºæ›¸é¡ã‹ã‚‰è²¡å‹™æƒ…å ±ã‚’æŠ½å‡ºãƒ»å¯è¦–åŒ–ã™ã‚‹ã‚¢ãƒ—ãƒª")

# ============================
# ğŸ§© docID ZIP â†’ XBRLèª­ã¿å–ã‚Š
# ============================

def extract_xbrl_from_zip(doc_id):
    url = f"https://disclosure.edinet-fsa.go.jp/api/v1/documents/{doc_id}"
    params = {"type": 1}
    res = requests.get(url, params=params, timeout=20, verify=False)

    content_type = res.headers.get("Content-Type", "")
    if "zip" in content_type:
        with zipfile.ZipFile(io.BytesIO(res.content)) as z:
            for file_name in z.namelist():
                if file_name.endswith(".xbrl"):
                    with z.open(file_name) as xbrl_file:
                        return xbrl_file.read().decode("utf-8")
        raise FileNotFoundError("XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    elif "pdf" in content_type:
        raise ValueError("ã“ã®docIDã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚Šã€XBRLãƒ‡ãƒ¼ã‚¿ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    elif "html" in content_type:
        raise ValueError("ã“ã®docIDã¯HTMLå½¢å¼ã§ã™ã€‚ç„¡åŠ¹ã¾ãŸã¯å…¬é–‹æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
    elif "json" in content_type:
        raise ValueError("ã“ã®docIDã¯XBRLç­‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æä¾›ã—ã¦ã„ã¾ã›ã‚“ã€‚åˆ¥ã®docIDã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
    else:
        raise ValueError(f"æœªçŸ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ï¼ˆContent-Type: {content_type}ï¼‰")

# ============================
# ğŸ“¥ docID â†’ CSVèª­ã¿è¾¼ã¿ï¼ˆAPI v2ï¼‰
# ============================

def fetch_csv_from_docid(doc_id):
    url = f"https://api.edinet-fsa.go.jp/api/v2/documents/{doc_id}"
    headers = {"Ocp-Apim-Subscription-Key": API_KEY}
    params = {"type": 5}  # CSVå–å¾—
    res = requests.get(url, headers=headers, params=params, timeout=20)

    content_type = res.headers.get("Content-Type", "")
    if "zip" not in content_type:
        raise ValueError(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆContent-Type: {content_type}ï¼‰")

    with zipfile.ZipFile(io.BytesIO(res.content)) as z:
        for file_name in z.namelist():
            if file_name.endswith(".csv"):
                with z.open(file_name) as f:
                    raw = f.read()
                    encoding = chardet.detect(raw)["encoding"]
                    return pd.read_csv(io.BytesIO(raw), encoding=encoding), file_name
    raise FileNotFoundError("CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒZIPå†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ============================
# ğŸ” XBRLã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
# ============================

def extract_financial_data_from_xbrl(xbrl_text):
    root = ET.fromstring(xbrl_text)
    ns = {"jp": "http://www.xbrl.go.jp/jp/fr/gaap/2023-03-31"}

    items = {
        "å£²ä¸Šé«˜": ["jp:NetSales", "jp:OperatingRevenue"],
        "å–¶æ¥­åˆ©ç›Š": ["jp:OperatingIncome"],
        "çµŒå¸¸åˆ©ç›Š": ["jp:OrdinaryIncome"],
        "ç´”åˆ©ç›Š": ["jp:ProfitAttributableToOwnersOfParent", "jp:NetIncome"]
    }

    results = {}
    for key, tags in items.items():
        for tag in tags:
            elem = root.find(f".//{tag}", ns)
            if elem is not None and elem.text:
                results[key] = elem.text
                break
        if key not in results:
            results[key] = "å–å¾—å¤±æ•—"
    return results

# ============================
# ğŸ” å…¨è²¡å‹™ã‚¿ã‚°ä¸€è¦§è¡¨ç¤º
# ============================

def list_all_financial_tags(xbrl_text):
    root = ET.fromstring(xbrl_text)
    tags = set()
    for elem in root.iter():
        if elem.tag.startswith("{http://www.xbrl.go.jp/jp/fr/gaap"):
            tag_clean = elem.tag.split("}")[-1]
            tags.add(tag_clean)
    return sorted(tags)

# ============================
# âœ… docIDãŒZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
# ============================

def is_zip_doc(doc_id):
    url = f"https://disclosure.edinet-fsa.go.jp/api/v1/documents/{doc_id}"
    try:
        res = requests.get(url, params={"type": 1}, timeout=10, verify=False)
        return "zip" in res.headers.get("Content-Type", "")
    except:
        return False

# ============================
# ğŸ” EDINETã‹ã‚‰æœ€æ–°docIDã‚’å–å¾—ï¼ˆZIPå½¢å¼ã®ã¿ï¼‰
# ============================

def fetch_recent_doc_ids(limit=20):
    results = []
    checked = 0
    date = datetime.today()

    while len(results) < limit and checked < 90:
        date -= timedelta(days=1)
        if date.weekday() >= 5:
            continue

        url = "https://disclosure.edinet-fsa.go.jp/api/v1/documents.json"
        params = {"date": date.strftime('%Y-%m-%d'), "type": 2}

        try:
            res = requests.get(url, params=params, timeout=10, verify=False)
            docs = res.json().get("results", [])
            for doc in docs:
                if doc.get("xbrlFlag") == "1" and is_zip_doc(doc["docID"]):
                    results.append({
                        "date": date.strftime('%Y-%m-%d'),
                        "docID": doc.get("docID"),
                        "filerName": doc.get("filerName"),
                        "docDescription": doc.get("docDescription")
                    })
                    if len(results) >= limit:
                        break
        except Exception as e:
            st.warning(f"{date.strftime('%Y-%m-%d')} ã®å–å¾—å¤±æ•—: {e}")
        checked += 1
    return results

# ============================
# Streamlit UI
# ============================

st.header("ğŸ“¥ docIDã‚’å…¥åŠ›ã—ã¦è²¡å‹™æƒ…å ±ã‚’å–å¾—")
doc_id = st.text_input("EDINETã®docIDã‚’å…¥åŠ›ï¼ˆä¾‹: S100UP32ï¼‰")

if st.button("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"):
    if not doc_id:
        st.warning("docIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.spinner("ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­..."):
            try:
                xbrl_text = extract_xbrl_from_zip(doc_id)
                data = extract_financial_data_from_xbrl(xbrl_text)
                st.success("âœ… æŠ½å‡ºæˆåŠŸï¼")
                for k, v in data.items():
                    st.write(f"{k}: {v}")
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if st.button("ğŸ” å…¨è²¡å‹™ã‚¿ã‚°ã‚’è¡¨ç¤º"):
    if not doc_id:
        st.warning("docIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.spinner("ã‚¿ã‚°æŠ½å‡ºä¸­..."):
            try:
                xbrl_text = extract_xbrl_from_zip(doc_id)
                tags = list_all_financial_tags(xbrl_text)
                st.success(f"ğŸ“„ ã‚¿ã‚°æ•°: {len(tags)} ä»¶")
                st.code("\n".join(tags))
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

st.header("ğŸ“„ æœ€æ–°ã®EDINETæå‡ºæ›¸é¡ï¼ˆXBRLã‚ã‚ŠdocIDä¸€è¦§ï¼‰")
if st.button("ğŸ“¥ ç›´è¿‘ã®docIDã‚’å–å¾—"):
    with st.spinner("æœ€æ–°æå‡ºæ›¸é¡ã‚’å–å¾—ä¸­..."):
        docs = fetch_recent_doc_ids(limit=30)
        if docs:
            for d in docs:
                st.write(f"{d['date']}ï½œ{d['filerName']}ï½œ{d['docDescription']}ï½œdocID: {d['docID']}")
        else:
            st.warning("docIDãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚é€šä¿¡ç’°å¢ƒã‚„æ—¥ä»˜ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

st.header("ğŸ“Š docIDã‹ã‚‰CSVè²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆAPIçµŒç”±ï¼‰")
csv_doc_id = st.text_input("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã™ã‚‹docIDï¼ˆä¾‹: S100UP32ï¼‰")

if st.button("CSVã‚’å–å¾—ï¼†è¡¨ç¤º"):
    if not csv_doc_id:
        st.warning("docIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("CSVãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­..."):
            try:
                df, fname = fetch_csv_from_docid(csv_doc_id)
                st.success(f"âœ… CSVå–å¾—æˆåŠŸ: {fname}")
                st.dataframe(df.head(30))
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
