import os
import io
import zipfile
import requests
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dotenv import load_dotenv

# --- åˆæœŸè¨­å®š ---
load_dotenv()
API_KEY = os.getenv("EDINET_API_KEY")
API_URL = "https://api.edinet-fsa.go.jp/api/v2"

st.title("ğŸ“¦ EDINET å››åŠæœŸå ±å‘Šæ›¸ ZIP â†’ è²¡å‹™æŒ‡æ¨™ã‚°ãƒ©ãƒ•")

if not API_KEY:
    st.error("APIã‚­ãƒ¼ãŒæœªè¨­å®šã§ã™")
    st.stop()

# --- é–¢æ•°å®šç¾© ---

# docID æ¤œç´¢ï¼ˆå››åŠæœŸå ±å‘Šæ›¸é™å®šï¼‰
def find_docid(company_name, days=90):
    headers = {"Ocp-Apim-Subscription-Key": API_KEY}
    today = datetime.today()

    for i in range(days):
        date = today - timedelta(days=i)
        if date.weekday() >= 5:
            continue
        try:
            res = requests.get(
                f"{API_URL}/documents.json",
                params={"date": date.strftime("%Y-%m-%d"), "type": 2},
                headers=headers,
                timeout=10
            )
            res.raise_for_status()
            for item in res.json().get("results", []):
                name = item.get("filerName")
                desc = item.get("docDescription")
                if name and desc and company_name in name and "å ±å‘Šæ›¸" in desc:
                    return item["docID"], desc
        except Exception:
            continue
    return None, None


# ZIPãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»è§£å‡
def download_and_extract_zip(doc_id, doc_type=5):
    headers = {"Ocp-Apim-Subscription-Key": API_KEY}
    res = requests.get(f"{API_URL}/documents/{doc_id}", params={"type": doc_type}, headers=headers, timeout=20)
    if "zip" not in res.headers.get("Content-Type", ""):
        return None

    folder = f"tmp_{doc_id}"
    os.makedirs(folder, exist_ok=True)

    with zipfile.ZipFile(io.BytesIO(res.content)) as z:
        z.extractall(folder)
    return folder

# CSVè§£æ
def parse_csv(folder_path):
    for fname in os.listdir(folder_path):
        if fname.endswith(".csv"):
            with open(os.path.join(folder_path, fname), "rb") as f:
                raw = f.read()
                encoding = "utf-8"  # fallback
                try:
                    import chardet
                    encoding = chardet.detect(raw)["encoding"]
                except:
                    pass
                df = pd.read_csv(io.BytesIO(raw), encoding=encoding)
                return df
    return None

# XBRLè§£æ
def parse_xbrl(folder_path):
    for fname in os.listdir(folder_path):
        if fname.endswith(".xbrl"):
            with open(os.path.join(folder_path, fname), "rb") as f:
                return f.read()
    return None

# è²¡å‹™æŒ‡æ¨™æŠ½å‡ºï¼ˆCSV or XBRLï¼‰
def extract_metrics(df=None, xml=None):
    keywords = {
        "å£²ä¸Šé«˜": ["NetSales", "NetSalesConsolidated"],
        "å–¶æ¥­åˆ©ç›Š": ["OperatingIncome", "OperatingIncomeConsolidated"],
        "çµŒå¸¸åˆ©ç›Š": ["OrdinaryIncome", "OrdinaryIncomeConsolidated"],
        "ç´”åˆ©ç›Š": ["NetIncome", "Profit", "NetIncomeAttributableToOwnersOfParent"],
    }
    results = {}

    if df is not None:
        for label, keys in keywords.items():
            found = pd.Series(dtype=str)
            for k in keys:
                found = df[df["é …ç›®ID"].astype(str).str.contains(k, na=False)]
                if not found.empty:
                    break
            results[label] = int(found.iloc[0]["é‡‘é¡"]) if not found.empty else None

    elif xml is not None:
        soup = BeautifulSoup(xml, "xml")
        for label, keys in keywords.items():
            val = None
            for tag in keys:
                el = soup.find(tag)
                if el and el.text.strip().isdigit():
                    val = int(el.text.strip())
                    break
            results[label] = val

    return results

# ã‚°ãƒ©ãƒ•æç”»
def show_graph(data):
    df = pd.DataFrame(data.items(), columns=["æŒ‡æ¨™", "é‡‘é¡"])
    df["é‡‘é¡"] = pd.to_numeric(df["é‡‘é¡"], errors="coerce")
    sns.set_style("whitegrid")
    plt.figure(figsize=(6, 4))
    sns.barplot(data=df, x="æŒ‡æ¨™", y="é‡‘é¡")
    plt.title("è²¡å‹™æŒ‡æ¨™")
    st.pyplot(plt.gcf())
    plt.clf()

# --- Streamlit UI ---
st.header("ä¼æ¥­åã‹ã‚‰ docID ã‚’æ¤œç´¢ â†’ è²¡å‹™ãƒ‡ãƒ¼ã‚¿å–å¾—")
company = st.text_input("ä¼æ¥­åã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾ï¼‰")

if st.button("å®Ÿè¡Œ"):
    if not company:
        st.warning("ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        st.stop()

    with st.spinner("docIDã‚’æ¤œç´¢ä¸­..."):
        doc_id, desc = find_docid(company)
        if not doc_id:
            st.error("è©²å½“ã™ã‚‹å››åŠæœŸå ±å‘Šæ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            st.stop()
        st.success(f"âœ… è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š{desc}ï½œdocID: {doc_id}")

    with st.spinner("ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ä¸­..."):
        folder = download_and_extract_zip(doc_id, doc_type=5)
        if not folder:
            st.warning("CSVå–å¾—ã«å¤±æ•—ã€‚XBRLã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
            folder = download_and_extract_zip(doc_id, doc_type=1)
            xml = parse_xbrl(folder)
            if not xml:
                st.error("XBRLã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.stop()
            metrics = extract_metrics(xml=xml)
        else:
            df = parse_csv(folder)
            if df is None:
                st.warning("CSVè§£æã«å¤±æ•—ã€‚XBRLã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
                folder = download_and_extract_zip(doc_id, doc_type=1)
                xml = parse_xbrl(folder)
                metrics = extract_metrics(xml=xml)
            else:
                metrics = extract_metrics(df=df)

    st.subheader("ğŸ“Š æŠ½å‡ºã•ã‚ŒãŸè²¡å‹™æŒ‡æ¨™")
    st.write(metrics)
    show_graph(metrics)
