import streamlit as st
import requests
import zipfile
import io
import os
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆï¼ˆWindowsï¼‰
plt.rcParams['font.family'] = 'MS Gothic'

# APIã‚­ãƒ¼
API_KEY = os.getenv("EDINET_API_KEY")
HEADERS = {"Ocp-Apim-Subscription-Key": API_KEY}
EDINET_API = "https://api.edinet-fsa.go.jp/api/v2"

st.title("ğŸ“Š EDINETå ±å‘Šæ›¸ã‹ã‚‰è²¡å‹™æŒ‡æ¨™ã‚’å¯è¦–åŒ–")

# docIDã‚’æ¤œç´¢
def find_latest_docid(company_name, days_back=180):
    today = datetime.today()
    target_types = {"120", "140", "160"}  # æœ‰å ±ã€å››å ±ã€åŠå ±

    for _ in range(days_back):
        today -= timedelta(days=1)
        if today.weekday() >= 5:
            continue
        try:
            res = requests.get(
                f"{EDINET_API}/documents.json",
                headers=HEADERS,
                params={"date": today.strftime('%Y-%m-%d'), "type": 2},
                timeout=10
            )
            results = res.json().get("results", [])
            for doc in results:
                if (
                    doc.get("filerName", "").strip().startswith(company_name)
                    and doc.get("docTypeCode", "") in target_types
                    and doc.get("csvFlag") == "1"  # ä»»æ„ï¼šCSVã‚‚ã‚ã‚‹ã‚‚ã®
                ):
                    return doc.get("docID"), doc.get("docDescription")
        except:
            continue
    return None, None


# XBRLæŠ½å‡º
def extract_xbrl_metrics(xml_data):
    soup = BeautifulSoup(xml_data, "xml")
    tag_map = {
        "å£²ä¸Šé«˜": ["NetSales", "NetSalesConsolidated"],
        "å–¶æ¥­åˆ©ç›Š": ["OperatingIncome", "OperatingIncomeConsolidated"],
        "çµŒå¸¸åˆ©ç›Š": ["OrdinaryIncome", "OrdinaryIncomeConsolidated"],
        "ç´”åˆ©ç›Š": ["NetIncome", "Profit", "NetIncomeAttributableToOwnersOfParent"]
    }
    result = {}
    for label, tags in tag_map.items():
        for tag in tags:
            el = soup.find(tag)
            if el and el.text.strip().isdigit():
                result[label] = int(el.text.strip())
                break
        if label not in result:
            result[label] = None
    return result

# ZIPã‹ã‚‰XBRLã‚’è§£å‡ï¼†å–å¾—
def fetch_xbrl_from_zip(doc_id):
    res = requests.get(
        f"{EDINET_API}/documents/{doc_id}",
        headers=HEADERS,
        params={"type": 1},
        timeout=15
    )
    if "zip" not in res.headers.get("Content-Type", ""):
        raise ValueError("ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    with zipfile.ZipFile(io.BytesIO(res.content)) as z:
        for name in z.namelist():
            if name.endswith(".xbrl") and "PublicDoc" in name:
                with z.open(name) as f:
                    return extract_xbrl_metrics(f.read())
    raise ValueError("XBRLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# ã‚°ãƒ©ãƒ•è¡¨ç¤º
def show_graph(metrics):
    df = pd.DataFrame(list(metrics.items()), columns=["æŒ‡æ¨™", "é‡‘é¡"])
    plt.figure(figsize=(6, 4))
    plt.bar(df["æŒ‡æ¨™"], df["é‡‘é¡"])
    plt.title("è²¡å‹™æŒ‡æ¨™")
    plt.ylabel("é‡‘é¡ï¼ˆç™¾ä¸‡å††ï¼‰")
    st.pyplot(plt)

# UI
company = st.text_input("ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾ï¼‰")
if st.button("å ±å‘Šæ›¸ã‚’æ¤œç´¢ã—ã¦å¯è¦–åŒ–"):
    if not company:
        st.warning("ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("docIDã‚’æ¤œç´¢ä¸­..."):
            doc_id, desc = find_latest_docid(company)
            if not doc_id:
                st.error("å ±å‘Šæ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.success(f"âœ… è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š{desc}ï½œdocID: {doc_id}")
                try:
                    metrics = fetch_xbrl_from_zip(doc_id)
                    st.write(metrics)
                    show_graph(metrics)
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
