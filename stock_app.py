import streamlit as st
import requests
import zipfile
import io
import pandas as pd
import chardet
import os
from datetime import datetime, timedelta
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import seaborn as sns
import japanize_matplotlib

# .env ã‹ã‚‰APIã‚­ãƒ¼å–å¾—
load_dotenv()
API_KEY = os.environ.get("EDINET_API_KEY")

st.title("ä¼æ¥­åã‹ã‚‰EDINETè²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ãƒ»å¯è¦–åŒ–")

if not API_KEY:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`.env` ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° 'EDINET_API_KEY' ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

API_ENDPOINT = "https://disclosure.edinet-fsa.go.jp/api/v2"
HEADERS = {"Ocp-Apim-Subscription-Key": API_KEY}

# ----------------------------
# docIDã‚’ä¼æ¥­åã§æ¤œç´¢ï¼ˆå››åŠæœŸå ±å‘Šæ›¸ã«é™å®šï¼‰
# ----------------------------
def search_docid_by_company_name(company_name, days_back=180):
    date = datetime.today()
    for _ in range(days_back):
        date -= timedelta(days=1)
        if date.weekday() >= 5:
            continue
        url = f"{API_ENDPOINT}/documents.json"
        params = {"date": date.strftime('%Y-%m-%d'), "type": 2}
        try:
            res = requests.get(url, headers=HEADERS, params=params, timeout=10)
            res.raise_for_status()
            for doc in res.json().get("results", []):
                name = doc.get("filerName", "")
                desc = doc.get("docDescription", "")
                csv_flag = doc.get("csvFlag", "0")
                if company_name in name and "å››åŠæœŸå ±å‘Šæ›¸" in desc:
                    return doc.get("docID"), name, desc, csv_flag
        except Exception:
            continue
    return None, None, None, "0"

# ----------------------------
# docIDã‹ã‚‰ZIPã‚’ä¿å­˜ãƒ»å±•é–‹
# ----------------------------
def save_csv(docID, type=5):
    st.info(f"{docID} ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­")
    params = {"type": type}
    r = requests.get(f"{API_ENDPOINT}/documents/{docID}", headers=HEADERS, params=params)
    if r.status_code != 200:
        st.warning("å–å¾—å¤±æ•—")
        return
    os.makedirs(docID, exist_ok=True)
    temp_zip = f"{docID}.zip"
    with open(temp_zip, "wb") as f:
        f.write(r.content)
    with zipfile.ZipFile(temp_zip) as z:
        z.extractall(docID)
    os.remove(temp_zip)

# ----------------------------
# CSVã‹ã‚‰è²¡å‹™æŒ‡æ¨™æŠ½å‡º
# ----------------------------
def extract_financial_metrics(df):
    if not set(["é …ç›®ID", "é‡‘é¡"]).issubset(df.columns):
        return {"ã‚¨ãƒ©ãƒ¼": "CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒä¸æ˜ã§ã™ï¼ˆå¿…è¦ãªåˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ï¼‰"}

    keywords = ["NetSales", "OperatingIncome", "OrdinaryIncome", "NetIncome"]
    extracted = {}
    for kw in keywords:
        matches = df[df["é …ç›®ID"].astype(str).str.contains(kw, na=False)]
        if not matches.empty:
            val = matches.iloc[0].get("é‡‘é¡", "")
            extracted[kw] = val
    return extracted

# ----------------------------
# XBRLã‹ã‚‰è²¡å‹™æŒ‡æ¨™æŠ½å‡º
# ----------------------------
def extract_metrics_from_xbrl(xml_content):
    soup = BeautifulSoup(xml_content, "xml")
    results = {}
    tag_map = {
        "NetSales": ["NetSales", "NetSalesConsolidated", "NetSalesOfReportingSegment"],
        "OperatingIncome": ["OperatingIncome", "OperatingIncomeConsolidated"],
        "OrdinaryIncome": ["OrdinaryIncome", "OrdinaryIncomeConsolidated"],
        "NetIncome": ["NetIncome", "Profit", "NetIncomeAttributableToOwnersOfParent"],
    }
    for label, tags in tag_map.items():
        for tag in tags:
            found = soup.find(tag)
            if found and found.text.strip().isdigit():
                results[label] = found.text.strip()
                break
        if label not in results:
            results[label] = "N/A"
    return results

# ----------------------------
# docIDã‹ã‚‰CSVã¾ãŸã¯XBRLã‚’å–å¾—ã—ã¦æŠ½å‡º
# ----------------------------
def fetch_data_by_docid(doc_id, use_csv=True):
    url = f"{API_ENDPOINT}/documents/{doc_id}"
    if use_csv:
        try:
            res = requests.get(url, headers=HEADERS, params={"type": 5}, timeout=15)
            if "zip" in res.headers.get("Content-Type", ""):
                with zipfile.ZipFile(io.BytesIO(res.content)) as z:
                    st.write("ZIPå†…ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:", z.namelist())
                    for file_name in z.namelist():
                        if file_name.endswith(".csv"):
                            with z.open(file_name) as f:
                                raw = f.read()
                                encoding = chardet.detect(raw)["encoding"] or "utf-8"
                                df = pd.read_csv(io.BytesIO(raw), encoding=encoding)
                                return extract_financial_metrics(df), "CSV"
        except Exception as e:
            st.warning(f"[CSVå–å¾—å¤±æ•—] {e}")
    try:
        res = requests.get(url, headers=HEADERS, params={"type": 1}, timeout=20)
        if "zip" in res.headers.get("Content-Type", ""):
            with zipfile.ZipFile(io.BytesIO(res.content)) as z:
                for file_name in z.namelist():
                    if "PublicDoc" in file_name and file_name.endswith(".xbrl"):
                        with z.open(file_name) as f:
                            xml_data = f.read()
                            return extract_metrics_from_xbrl(xml_data), "XBRL"
    except Exception as e:
        st.warning(f"[XBRLå–å¾—å¤±æ•—] {e}")
    raise ValueError("CSVãƒ»XBRLã¨ã‚‚ã«å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

# ----------------------------
# æ¯”è¼ƒç”¨ã‚°ãƒ©ãƒ•æç”»é–¢æ•°
# ----------------------------
def compare_company_IR(data, contextId, elementId, elementJpName):
    plot_data = data.query(f"è¦ç´ ID == '{elementId}' and ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆID == '{contextId}'").copy()
    plot_data[elementJpName] = pd.to_numeric(plot_data["å€¤"], errors="coerce")
    sns.barplot(data=plot_data, x="ä¼šç¤¾å", y=elementJpName)
    plt.title(elementJpName)
    plt.ylabel(elementJpName)
    plt.xticks(rotation=30)
    plt.tight_layout()
    st.pyplot(plt)
    plt.clf()

# ----------------------------
# Streamlit UI
# ----------------------------
st.header("ä¼æ¥­åã‹ã‚‰EDINETè²¡å‹™ãƒ‡ãƒ¼ã‚¿æ¤œç´¢")
company = st.text_input("ä¼æ¥­åã‚’å…¥åŠ›ï¼ˆä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾ï¼‰")

if st.button("æ¤œç´¢ã—ã¦è²¡å‹™ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º"):
    if not company:
        st.warning("ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        with st.spinner("EDINETã§docIDæ¤œç´¢ä¸­..."):
            doc_id, name, desc, csv_flag = search_docid_by_company_name(company)
            if not doc_id:
                st.error("docIDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆå¯¾è±¡æ›¸é¡ãŒãªã„å¯èƒ½æ€§ï¼‰")
            else:
                st.success(f"è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š{name}ï½œ{desc}ï½œdocID: {doc_id}ï½œCSV: {csv_flag}")
                try:
                    metrics, source = fetch_data_by_docid(doc_id, use_csv=(csv_flag == "1"))
                    if all(v == "N/A" or v == "" for v in metrics.values()):
                        st.warning("æ›¸é¡ã«è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                    else:
                        st.subheader(f"æŠ½å‡ºã•ã‚ŒãŸè²¡å‹™æŒ‡æ¨™ï¼ˆ{source}ã‹ã‚‰å–å¾—ï¼‰")
                        result_df = pd.DataFrame([{"æŒ‡æ¨™": k, "é‡‘é¡": v} for k, v in metrics.items()])
                        st.table(result_df)
                except Exception as e:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# ----------------------------
# è¤‡æ•°ä¼æ¥­ã®æ¯”è¼ƒï¼šEPSã‚„ç²—åˆ©ç›Š
# ----------------------------
docID_dict = {
    "å•†èˆ¹ä¸‰äº•": "S100STH6",
    "æ—¥æœ¬éƒµèˆ¹": "S100SS7P",
    "ç‰äº•å•†èˆ¹æ ªå¼ä¼šç¤¾": "S100STLS",
    "å·å´æ±½èˆ¹": "S100SRTI",
    "é£¯é‡æµ·é‹": "S100SP9O",
}

all_data = pd.DataFrame()
for company, docID in docID_dict.items():
    save_csv(docID, type=5)
    folder = f"{docID}/XBRL/PublicDoc"
    if not os.path.exists(folder):
        continue
    for file in os.listdir(folder):
        if file.endswith(".csv"):
            df = pd.read_csv(os.path.join(folder, file), encoding="utf-8", low_memory=False)
            df["ä¼šç¤¾å"] = company
            all_data = pd.concat([all_data, df], ignore_index=True)

st.subheader("ğŸ“Š EPSæ¯”è¼ƒ")
compare_company_IR(
    all_data,
    "CurrentQuarterDuration",
    "jpcrp_cor:BasicEarningsLossPerShareSummaryOfBusinessResults",
    "EPS"
)

st.subheader("ğŸ“Š ç²—åˆ©ç›Šæ¯”è¼ƒ")
compare_company_IR(
    all_data,
    "CurrentYTDDuration",
    "jppfs_cor:GrossProfit",
    "ç²—åˆ©ç›Š"
)
