import streamlit as st
import os
import requests
import zipfile
import io
import pandas as pd
import chardet
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()
API_KEY = os.environ.get("EDINET_API_KEY")

st.title("ğŸ“Š ä¼æ¥­åã‹ã‚‰EDINETè²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ãƒ»å¯è¦–åŒ–")

if not API_KEY:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`.env` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# ----------------------------
# ğŸ” docIDæ¤œç´¢ï¼ˆå››åŠæœŸå ±å‘Šæ›¸å„ªå…ˆï¼‰
# ----------------------------
def find_docid(company_name, days=180):
    headers = {"Ocp-Apim-Subscription-Key": API_KEY}
    date = datetime.today()
    for _ in range(days):
        date -= timedelta(days=1)
        if date.weekday() >= 5:
            continue
        url = "https://api.edinet-fsa.go.jp/api/v2/documents.json"
        params = {"date": date.strftime("%Y-%m-%d"), "type": 2}
        try:
            res = requests.get(url, headers=headers, params=params, timeout=10)
            res.raise_for_status()
            for item in res.json().get("results", []):
                if "å››åŠæœŸå ±å‘Šæ›¸" in item.get("docDescription", "") and company_name in item.get("filerName", ""):
                    return item["docID"], item["docDescription"]
        except Exception:
            continue
    return None, None

# ----------------------------
# ğŸ“¥ ZIPå–å¾—ã¨è§£å‡ â†’ CSV or XBRLå‡¦ç†
# ----------------------------
def extract_from_zip(doc_id):
    headers = {"Ocp-Apim-Subscription-Key": API_KEY}
    url_csv = f"https://api.edinet-fsa.go.jp/api/v2/documents/{doc_id}?type=5"
    url_xbrl = f"https://api.edinet-fsa.go.jp/api/v2/documents/{doc_id}?type=1"

    # CSV
    try:
        res = requests.get(url_csv, headers=headers, timeout=15)
        if "zip" in res.headers.get("Content-Type", ""):
            with zipfile.ZipFile(io.BytesIO(res.content)) as z:
                for file in z.namelist():
                    if file.endswith(".csv"):
                        raw = z.read(file)
                        encoding = chardet.detect(raw)["encoding"]
                        df = pd.read_csv(io.BytesIO(raw), encoding=encoding)
                        return parse_csv(df), "CSV"
    except Exception:
        pass

    # XBRL fallback
    try:
        res = requests.get(url_xbrl, headers=headers, timeout=20)
        if "zip" in res.headers.get("Content-Type", ""):
            with zipfile.ZipFile(io.BytesIO(res.content)) as z:
                for file in z.namelist():
                    if "PublicDoc" in file and file.endswith(".xbrl"):
                        xml = z.read(file)
                        return parse_xbrl(xml), "XBRL"
    except Exception as e:
        st.warning(f"XBRLå–å¾—å¤±æ•—: {e}")

    return None, "å–å¾—å¤±æ•—"

# ----------------------------
# ğŸ“‘ CSVè§£æ
# ----------------------------
def parse_csv(df):
    if not set(["é …ç›®ID", "é‡‘é¡"]).issubset(df.columns):
        return {"ã‚¨ãƒ©ãƒ¼": "CSVåˆ—ãŒè¶³ã‚Šã¾ã›ã‚“"}
    keys = ["NetSales", "OperatingIncome", "OrdinaryIncome", "NetIncome"]
    out = {}
    for k in keys:
        matches = df[df["é …ç›®ID"].astype(str).str.contains(k, na=False)]
        if not matches.empty:
            out[k] = matches.iloc[0]["é‡‘é¡"]
    return out

# ----------------------------
# ğŸ“‘ XBRLè§£æ
# ----------------------------
def parse_xbrl(xml):
    soup = BeautifulSoup(xml, "xml")
    tag_map = {
        "NetSales": ["NetSales", "NetSalesConsolidated"],
        "OperatingIncome": ["OperatingIncome"],
        "OrdinaryIncome": ["OrdinaryIncome"],
        "NetIncome": ["NetIncome", "Profit"]
    }
    result = {}
    for label, tags in tag_map.items():
        for tag in tags:
            found = soup.find(tag)
            if found and found.text.strip().isdigit():
                result[label] = found.text.strip()
                break
        if label not in result:
            result[label] = "N/A"
    return result

# ----------------------------
# ğŸ“Š ã‚°ãƒ©ãƒ•æç”»
# ----------------------------
def plot_metrics(metrics):
    df = pd.DataFrame(metrics.items(), columns=["æŒ‡æ¨™", "é‡‘é¡"])
    df["é‡‘é¡"] = pd.to_numeric(df["é‡‘é¡"], errors="coerce")
    sns.barplot(x="æŒ‡æ¨™", y="é‡‘é¡", data=df)
    st.pyplot(plt.gcf())
    plt.clf()

# ----------------------------
# Streamlit UI
# ----------------------------
st.header("ğŸ” ä¼æ¥­åã‚’å…¥åŠ›")
company = st.text_input("ä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾")

if st.button("æ¤œç´¢"):
    if not company:
        st.warning("ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("docIDã‚’æ¤œç´¢ä¸­..."):
            doc_id, desc = find_docid(company)
            if not doc_id:
                st.error("å››åŠæœŸå ±å‘Šæ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.success(f"âœ… {desc}ï½œdocID: {doc_id}")
                with st.spinner("ZIPãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ãƒ»è§£å‡ä¸­..."):
                    data, src = extract_from_zip(doc_id)
                    if data:
                        st.subheader(f"ğŸ“ˆ {src}ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸè²¡å‹™ãƒ‡ãƒ¼ã‚¿")
                        st.dataframe(pd.DataFrame(data.items(), columns=["æŒ‡æ¨™", "é‡‘é¡"]))
                        plot_metrics(data)
                    else:
                        st.error("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
