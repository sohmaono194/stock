import streamlit as st
import requests
import zipfile
import io
import pandas as pd
import chardet
import os
from datetime import datetime, timedelta
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆCloudå‘ã‘ï¼šNotoã‚„IPAãªã©ãŒæœ‰åŠ¹ãªå¯èƒ½æ€§ã‚ã‚Šï¼‰
plt.rcParams['font.family'] = 'Noto Sans CJK JP'

# .env èª­ã¿è¾¼ã¿
load_dotenv()
API_KEY = os.environ.get("EDINET_API_KEY")
API_ENDPOINT = "https://disclosure.edinet-fsa.go.jp/api/v2"

st.title("ğŸ“Š ä¼æ¥­åã‹ã‚‰EDINETè²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ãƒ»ã‚°ãƒ©ãƒ•åŒ–")

if not API_KEY:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`.env` ã« 'EDINET_API_KEY' ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# docID æ¤œç´¢ï¼ˆå››åŠæœŸå ±å‘Šæ›¸ã‚’å„ªå…ˆï¼‰
def search_docid(company_name, days_back=180):
    date = datetime.today()
    headers = {"Ocp-Apim-Subscription-Key": API_KEY}
    for _ in range(days_back):
        date -= timedelta(days=1)
        if date.weekday() >= 5:
            continue
        url = f"{API_ENDPOINT}/documents.json"
        params = {"date": date.strftime("%Y-%m-%d"), "type": 2}
        try:
            res = requests.get(url, headers=headers, params=params, timeout=10)
            res.raise_for_status()
            for doc in res.json().get("results", []):
                if company_name in doc.get("filerName", "") and "å››åŠæœŸå ±å‘Šæ›¸" in doc.get("docDescription", ""):
                    return doc.get("docID"), doc.get("filerName"), doc.get("docDescription"), doc.get("csvFlag", "0")
        except:
            continue
    return None, None, None, "0"

# CSVã‹ã‚‰æŒ‡æ¨™ã‚’æŠ½å‡º
def extract_from_csv(df):
    if not set(["é …ç›®ID", "é‡‘é¡"]).issubset(df.columns):
        return {}
    keywords = {
        "å£²ä¸Šé«˜": "NetSales",
        "å–¶æ¥­åˆ©ç›Š": "OperatingIncome",
        "çµŒå¸¸åˆ©ç›Š": "OrdinaryIncome",
        "ç´”åˆ©ç›Š": "NetIncome"
    }
    results = {}
    for jp, en in keywords.items():
        match = df[df["é …ç›®ID"].astype(str).str.contains(en, na=False)]
        if not match.empty:
            results[jp] = int(match.iloc[0]["é‡‘é¡"])
    return results

# XBRLã‹ã‚‰æŒ‡æ¨™ã‚’æŠ½å‡º
def extract_from_xbrl(xml):
    soup = BeautifulSoup(xml, "xml")
    tags = {
        "å£²ä¸Šé«˜": ["NetSales", "NetSalesConsolidated"],
        "å–¶æ¥­åˆ©ç›Š": ["OperatingIncome", "OperatingIncomeConsolidated"],
        "çµŒå¸¸åˆ©ç›Š": ["OrdinaryIncome", "OrdinaryIncomeConsolidated"],
        "ç´”åˆ©ç›Š": ["NetIncome", "NetIncomeAttributableToOwnersOfParent"]
    }
    result = {}
    for label, options in tags.items():
        for tag in options:
            found = soup.find(tag)
            if found and found.text.strip().isdigit():
                result[label] = int(found.text.strip())
                break
        if label not in result:
            result[label] = None
    return result

# docIDã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
def fetch_metrics(doc_id, use_csv=True):
    headers = {"Ocp-Apim-Subscription-Key": API_KEY}
    url = f"{API_ENDPOINT}/documents/{doc_id}"

    if use_csv:
        try:
            res = requests.get(url, headers=headers, params={"type": 5}, timeout=15)
            if "zip" in res.headers.get("Content-Type", ""):
                with zipfile.ZipFile(io.BytesIO(res.content)) as z:
                    for name in z.namelist():
                        if name.endswith(".csv"):
                            raw = z.read(name)
                            enc = chardet.detect(raw)["encoding"]
                            df = pd.read_csv(io.BytesIO(raw), encoding=enc)
                            return extract_from_csv(df), "CSV"
        except Exception as e:
            st.warning(f"[CSVã‚¨ãƒ©ãƒ¼] {e}")

    # fallback to XBRL
    try:
        res = requests.get(url, headers=headers, params={"type": 1}, timeout=20)
        if "zip" in res.headers.get("Content-Type", ""):
            with zipfile.ZipFile(io.BytesIO(res.content)) as z:
                for name in z.namelist():
                    if name.endswith(".xbrl"):
                        xml = z.read(name)
                        return extract_from_xbrl(xml), "XBRL"
    except Exception as e:
        st.warning(f"[XBRLã‚¨ãƒ©ãƒ¼] {e}")

    return {}, "å–å¾—å¤±æ•—"

# ã‚°ãƒ©ãƒ•æç”»
def plot_metrics(metrics, company_name):
    labels = list(metrics.keys())
    values = list(metrics.values())

    fig, ax = plt.subplots()
    ax.bar(labels, values)
    ax.set_title(f"{company_name} ã®è²¡å‹™æŒ‡æ¨™")
    ax.set_ylabel("é‡‘é¡ï¼ˆç™¾ä¸‡å††ï¼‰")
    plt.xticks(rotation=30)
    st.pyplot(fig)

# UI
st.header("ğŸ” ä¼æ¥­åã‹ã‚‰æ¤œç´¢")
company = st.text_input("ä¾‹: ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾")

if st.button("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»ã‚°ãƒ©ãƒ•è¡¨ç¤º"):
    if not company:
        st.warning("ä¼æ¥­åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("EDINETã‹ã‚‰docIDæ¤œç´¢ä¸­..."):
            docID, name, desc, csv_flag = search_docid(company)
        if not docID:
            st.error("docIDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.success(f"âœ… è¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š{name}ï½œ{desc}ï½œdocID: {docID}ï½œCSVå¯¾å¿œ: {csv_flag}")
            metrics, source = fetch_metrics(docID, use_csv=(csv_flag == "1"))
            if not metrics:
                st.error("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.subheader(f"ğŸ“Š æŠ½å‡ºçµæœï¼ˆ{source}ï¼‰")
                st.dataframe(pd.DataFrame(metrics.items(), columns=["æŒ‡æ¨™", "é‡‘é¡"]))
                plot_metrics(metrics, name)
